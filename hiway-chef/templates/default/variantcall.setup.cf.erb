#!/usr/bin/env cuneiform

/* VARIANT-CALL-SETUP
 *
 * Software dependencies:
 *
 * Annovar
 * Bowtie2 2.2.2
 * Perl 5.14.2
 * Hadoop 2.2.0
 *
 * Memory dependency: 2048 MB
 *
 */

deftask annovar-downdb( db(File) fa(File) : buildver query ) in bash *{
    fa=hg19_refGeneMrna.fa
    db=db.tar
    annotate_variation.pl -buildver $buildver -downdb -webfrom annovar $query db/
    cp db/$fa $fa
    tar cf $db db
}*

// bowtie2
deftask bowtie2-idx( idx(File) : fa(File) ) in bash *{
    idx=$fa.tar
    bowtie2-build $fa bowtie2-index
    mv $fa bowtie2-index.fa
    tar cf $idx --remove-files bowtie2-index.*
}*

deftask hdfs-copy-from-local( dependency : file(File) dir ) in bash *{
    hdfs dfs -mkdir -p $dir
    for job in `jobs -p`; do wait $job; done
    hdfs dfs -copyFromLocal -f $file $dir/`echo $file | sed 's/^[0-9_]*_//'`
    for job in `jobs -p`; do wait $job; done
	dependency=$file
}*

deftask hdfs-ls( list(File) : pattern dir <dependency> ) in bash *{
    # dependency
    list=$dir$pattern.txt
    out=`hdfs dfs -ls $dir | tail -n +2 | grep -o -P "[^\s]*$pattern[^\s]*$"`
    for job in `jobs -p`; do wait $job; done
    echo $out > $list
    hdfs dfs -copyFromLocal -f $list `echo $list | sed 's/^[0-9_]*_//'`
    for job in `jobs -p`; do wait $job; done
}*

deftask wget( file(File) : path ) in bash *{
    file=`basename $path`
    wget $path -O $file
}*

deftask parse-index( <path> : index(File) n ) in bash *{
    path=$(grep '_[12]\.filt\.fastq\.gz' $index | grep -P '\tILLUMINA\t' | grep -P '\t0\t' | cut -f1 | sed -e 's/^/ftp.1000genomes.ebi.ac.uk\/vol1\/ftp\//' | tail -n `expr 2 \* $n`)
}*

deftask gunzip( out(File) : gz(File) ) in bash *{
    out=${gz%.gz}
    gzip -d -c -S .tar.gz $gz > $out
}*

deftask split( <out(File)> : file(File) lines ) in bash *{
    if [ `wc -l $file | awk '{ print $1 }'` -ge $lines ]
    then
        split -d -l `expr 4 \* $lines` -a 6 $file ${file}_split
        out=(${file}_split*)
    else
        out=$file
    fi
}*

// 2 splits (splitsize-reference ~= 23511 / desired dop (rounded up)
splitsize-reference = '11756';

// 10281156 Byte - Number of (paired) read files to download and align (about 800 MB per File)
n-reads = '1';

db ref = annovar-downdb(
    query    : 'refGene'
    buildver : 'hg19'
);

annovardb = hdfs-copy-from-local(
    file  : db
    dir   : 'annovar'
);

reference = hdfs-copy-from-local(
    file  : ref
    dir   : 'reference'
);

fa-split = split (
    file  : ref
    lines : splitsize-reference
);

idx = bowtie2-idx(
    fa : fa-split
);

index = hdfs-copy-from-local(
    file  : idx
    dir   : 'index'
);

index-list = hdfs-ls(
    pattern    : ''
    dir        : 'index'
    dependency : index
);

sequence-index = wget(
    path : 'ftp.1000genomes.ebi.ac.uk/vol1/ftp/sequence.index'
);

fastq-gz-path = parse-index(
    index : sequence-index
    n     : n-reads
);

fastq-gz = wget(
    path : fastq-gz-path
);

fastq = gunzip(
    gz : fastq-gz
);

reads = hdfs-copy-from-local(
    file  : fastq
    dir   : 'reads'
);

reads1-list = hdfs-ls(
    pattern    : '_1'
    dir        : 'reads'
	dependency : reads
);

reads2-list = hdfs-ls(
    pattern    : '_2'
    dir        : 'reads'
	dependency : reads
);

reference index-list annovardb reads1-list reads2-list;
